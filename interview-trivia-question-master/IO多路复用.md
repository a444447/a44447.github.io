假如我们要设计一个网络服务器，它会面临很多客户端的连接请求，有什么设计思路？

第一种显而易见的，就是每有一个连接请求就建立一个线程来单独处理。这样的方法可能会创建很多的线程，而线程之间的切换涉及了上下文切换，对CPU的代价较高。

## I/O多路复用

I/O多路复用的思想和CPU的并发多个进程很像。既然为每个连接分配一个线程/进程的方式不适合，那么如果我们只用一个进程来处理很多个请求，把对处理每个时间的控制在1ms，那么1分钟就可以处理上千个请求。看起来就像多个请求都复用了一个进程。

### select/poll

select实现多路复用的方式是，把所有已经连接的socket都放入一个 **文件描述符**集合，然后调用`select`方法把这个文件描述集拷贝到内核中。内核来检查是否有网络事件产生，检查的方式就是 **直接遍历文件描述符集合**。当检查到有事件产生，就把这个socket标为可读或可写，然后再把这个文件描述符拷贝回用户态。用户态还需要 遍历这个文件描述符集合找到可读可写的socket。

select中，使用的是固定长度的BitsMap表示文件描述符集合，具体长度由`FD_SETSIZE`决定，默认是1024.

poll不再使用BitsMap表示文件描述符集合，而是使用动态数组以链表来组织。

**select/poll都是使用「线性结构」存储进程关注的socket集合，因此都需要遍历来找到可读或可写的socket，复杂度都是O(n)**

### epoll

epoll的大概用法是这样的：`epoll_create`创建一个epoll对象`epfd`，然后通过`epoll_ctl`把需要监视的socket放入`epfd`中，然后调用`epoll_wait`等待数据

epoll相比select有很多改进

+ epoll内核里面使用红黑树来跟踪所有待检测的文件描述字，`epoll_ctl`就是把需要监控的socket放入epoll_ctl()中。红黑树的增删查改时间一般是O(logn)。而且相比于`select/poll`它们在内核中没有维护一个像红黑树这样保存所有待检测socket的数据结构，所以它们每次都需要复制整个socket集合给内核。而epoll中在内核维护了红黑树，于是每次只需要传入一个待检测的socket。
+ epoll是 *事件驱动*的，内核里面维护了一个链表来记录就绪事件。当某个socket有事件发生，就会通过 *回调函数*内核将其加入到这个就绪事件列表中。用户调用`epoll_wait`只会返回有事件发生的文件描述个数，而不用和`selec/poll`一样去轮询整个socket集合。

epoll能解决C10K问题。

**边缘触发和水平触发**

epoll支持两种事件触发模式，分别是边缘触发（ET）和水平触发（LT）

+ 边缘触发：当被监控的socket有可读事件发生的时候，服务器端只会从epoll_wait苏醒一次，即使进程没有调用read从内核去读，也只会苏醒一次。因此要抱着一次性将内核缓冲区的数据读取完。
+ 水平触发：当被监控的socket有可读事件发生，服务器会不断从epoll_wait中苏醒。直到内核缓冲区数据被read函数读完才结束。

边缘触发一般会搭配非阻塞I/O搭配。

`select/poll`只有水平触发模式，epoll默认是水平触发。

---

综合上面的，总结,epoll至少需要两个集合：

+ 一个所有fd集合
+ 就绪fd集合

我们知道对于一个fd,底层对应一个TCB，所以有key=fd,val=TCB这样的典型kv数据结构,kv型数据结构一般可以用:

1. hash
2. 红黑树
3. b/b+树

如果是哈希表的话查询速度很快O(1)，但是调用`epoll_create`的时候,底层的hash数组要创建多大呢？如果我们有百万的fd，那么这个数组越大越好，如果我们仅仅十几个fd需要管理，在创建数组的时候，太大的空间就很浪费。而这个fd我们又不能预先知道有多少，所以hash是不合适的。

b/b+树主要是做磁盘索引的，在内存场景不太合适。

对于内存场景，最好的还是红黑树，它是O(logn)的查找速度，其次在创建的时候只需要创建一个红黑树根就行。

那么就绪集合用什么数据结构呢，首先就绪集合不是以查找为主的，就绪集合的作用是将里面的元素拷贝给用户进行处理，所以集合里的元素没有优先级，那么就可以采用线性的数据结构，使用队列来存储，先进先出，先就绪的先处理。

所有fd的总集 -----> 红黑树

就绪fd的集合 -----> 队列

**红黑树的节点和就绪队列的节点是同一个节点**，所谓加入就绪队列其实是把指针联系了起来。所以就绪了并不是把红黑树中的节点删除了然后加入队列。

![协议栈如何与epoll模块通信](https://img-blog.csdnimg.cn/bb2928df96f347dfa5281058354bcf01.png)



应用程序只能通过三个api接口来操作epoll。当一个io准备就绪的时候，epoll是怎么知道io准备就绪了呢？是由协议栈将数据解析出来触发回调通知epoll的。也就是说可以把epoll的工作环境看出三部分，左边应用程序的api，中间的epoll，右边是协议栈的回调(协议栈当然不能直接操作epoll，中间的vfs在此不是重点，就直接省略vfs这一层了)。

![img](https://img-blog.csdnimg.cn/fe3a42ffedf44835ba8cd4048c8b042b.png)

---

epoll的回调事件是由协议栈触发的。epoll如何知道是哪个IO就绪了？

们从ip头可以解析出源ip，目的ip和协议，从tcp头可以解析出源端口和目的端口，此时五元组就凑齐了。socket **fd --- < 源IP地址 , 源端口 , 目的IP地址 , 目的端口 , 协议 > 一个fd就是一个五元组**，知道了fd，我们就能从红黑树中找到对应的结点。

那么这个回调函数做什么事情呢？我们传入fd和具体事件这两个参数，然后做下面两个操作

1. 通过fd找到对应的结点
2. 把结点加入到就绪队列

![img](https://img-blog.csdnimg.cn/ef11bb0cf33b48968775ce77a88d84f4.png)

一个有5个通知的地方

1. 三次握手完成之后，
2. 接收数据回复ACK之后，
3. 发送数据收到ACK之后，
4. 接收FIN回复ACK之后，
5. 接收RST回复ACK之后

---

**我们现在从回调的角度来看看select/poll与epoll的区别**。

每次调用select/poll，都需要把文件描述符集合拷贝到内核态，检测完之后，再有内核态拷贝的用户态，这就是poll。

而epoll不是这样，epoll只要有新的io就调用epoll_ctl()加入到红黑树里面，一旦有触发就用epoll_wait()将有事件的结点带出来。可以看到 **epoll就是只拷贝需要的东西**

还有个区别就是，epoll的事件是通过协议栈进行回调然后加入到队列中，而select/poll检测IO是否就绪只能通过遍历的方法。

epoll_create() ===》创建红黑树的根节点

epoll_ctl() ===》add,del,mod 增加、删除、修改结点

epoll_wait() ===》把就绪队列的结点copy到用户态放到events里面，跟recv函数很像

![img](https://img-blog.csdnimg.cn/a1ac62df78354f2bbb823e6c03bb8224.png)

**分析加锁**

如果有3个线程同时操作epoll，有哪些地方需要加锁？我们用户层面一共就只有3个api可以使用：

如果同时调用 epoll_create() ，那就是创建三颗红黑树，没有涉及到资源竞争，没有关系。

如果同时调用 epoll_ctl() ，对同一颗红黑树进行，增删改，这就涉及到资源竞争需要加锁了，此时我们对整棵树进行加锁。

如果同时调用epoll_wait() ，其操作的是就绪队列，所以需要对就绪队列进行加锁。

我们要扣住epoll的工作环境，在应用程序调用 epoll_ctl() ，协议栈会不会有回调操作红黑树结点？调用epoll_wait() copy出来的时候，协议栈会不会操作操作红黑树结点加入就绪队列？综上所述：

```bash
epoll_ctl() 对红黑树加锁
epoll_wait()对就绪队列加锁
回调函数()   对红黑树加锁,对就绪队列加锁
```

那么红黑树加什么锁，就绪队列加什么锁呢？

对于红黑树这种节点比较多的时候，采用互斥锁来加锁。

就绪队列就跟生产者消费者一样，结点是从协议栈回调函数来生产的，消费是epoll_wait()来消费。那么对于队列而言，用自旋锁（对于队列而言，插入删除比较简单，cpu自旋等待比让出的成本更低，所以用自旋锁）。

### 事件通知方式



- `select`

  - 需要遍历整个 `fd_set` 来找到触发事件的 `fd`，时间复杂度 `O(n)`。

- `epoll`

  :

  - 事件发生后，内核直接将触发的 `fd` 传递给用户态，避免了不必要的遍历，时间复杂度接近 `O(1)`。





## Reactor

I/O多路复用编写是一个面向过程的方式，而Reactor模式是基于面向对象的思想，对I/O复用作了一层封装，让使用者不用考虑底层网络API的细节，只用关注应用代码的编写。

> Reactor这个名字是反应堆，意思是 **对事件反应**，也就是来了一个事件就有对应的反应。其实就是I/O多路复用，收到事件后，根据事件分配给某个线程

所以Reactor模式就有Reactor和处理事件池两个部分

+ Reactor负责监听和分发时间，包括连接事件、读写事件；
+ 处理资源池负责处理事件

> 讲一下阻塞、非阻塞、同步、异步I/O概念
>
> **阻塞**： 当程序执行`read`，线程会阻塞，等待内核准备好数据并拷贝回应用程序的缓冲区，拷贝完成后才会返回
>
> **非阻塞I/O**: 在调用`read`的时候，数据没准备好回立刻返回，继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核把数据拷贝到应用程序缓冲区,read调用获得结果。
>
> ![非阻塞 I/O](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20.png)
>
> **注意，最后一次read调用是一个同步的过程，是需要等待内核态数据拷贝到应用程序缓冲区的。**
>
> 我们说无论是阻塞还是非阻塞I/O都是同步调用，因为read调用的时候，从内核空间拷贝到应用程序缓冲区都需要等待。
>
> 而真正的 *异步I/O*是 **内核准备好数据**、**数据从内核态拷贝到用户态**都不需要等待的。当我们发起异步I/O之后就立马返回，内核自动把数据拷贝到用户态，拷贝操作也是异步的——与前面不同，应用程序不需要主动发起拷贝动作。
>
> ![异步 I/O](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%BC%82%E6%AD%A5%20I_O.png)



Reactor模式主要有两个核心部分

+ Reactor负责监听和分发事件
+ 有一个资源池负责处理事件

常见的Reactor实现模式有

+ 单Reactor 单进程/线程
+ 单Reactor 多进程/线程
+ 多Reactor 多进程/线程

