[分布式相关面试题先来看看简单的分布式系统机架构图 初步了解一下分布式系统是个什么玩意 为什么要将系统进行拆分 要是不拆分 - 掘金](https://juejin.cn/post/6996915693811662884)





## 分布式缓存

分布式缓存是一种 **存储经常访问的数据，以减少数据库查询负担并加速系统响应** 的方案。它主要用于：

- **降低数据库压力**
- **提高系统吞吐量**
- **减少请求延迟**
- **应对高并发访问**

常见的分布式缓存系统包括：

- **Redis**（单节点、主从、集群模式）
- **Memcached**（简单 KV 存储）

### 分布式缓存的关键问题

#### **缓存穿透**

**问题**：查询一个数据库中 **不存在的键**，每次都查询数据库，导致缓存失效，数据库压力增加。

**解决方案**：

- **缓存空值**：查询不到数据时，缓存一个 `NULL` 值，并设定短时间过期。
- **布隆过滤器（Bloom Filter）**：使用 **布隆过滤器** 拦截不存在的 Key。

#### 缓存击穿

**热点数据** 在某一时刻失效，导致大量请求同时访问数据库，造成瞬时压力。

**解决方案**：

- **互斥锁**：如果缓存失效，只有一个线程能访问数据库，其他线程等待。
- **提前更新缓存**：使用 **定时任务** 提前刷新缓存。
- **热点数据永不过期**，然后后台 **异步更新**。

#### **缓存雪崩**

**问题**：**大量缓存同时过期**，导致数据库瞬时承受巨大请求，可能引发崩溃。

**解决方案**：

- **缓存过期时间加随机数**，避免大量缓存同时失效。
- **双缓存策略**：新老缓存并存，逐步淘汰。
- **预热缓存**：提前将热点数据写入缓存，避免缓存突然失效。

### 一致性哈希

对于分布式缓存来说，当一个节点接收到请求，如果该节点并没有存储缓存值，那么它面临的难题是，从谁那获取数据？自己，还是节点1, 2, 3, 4… 。假设包括自己在内一共有 10 个节点，当一个节点接收到请求时，随机选择一个节点，由该节点从数据源获取数据。

假设第一次随机选取了节点 1 ，节点 1 从数据源获取到数据的同时缓存该数据；那第二次，只有 1/10 的可能性再次选择节点 1, 有 9/10 的概率选择了其他节点，如果选择了其他节点，就意味着需要再一次从数据源获取数据，一般来说，这个操作是很耗时的。这样做，一是缓存效率低，二是各个节点上存储着相同的数据，浪费了大量的存储空间。

那有什么办法，对于给定的 key，每一次都选择同一个节点呢？使用 hash 算法也能够做到这一点。那把 key 的每一个字符的 ASCII 码加起来，再除以 10 取余数可以吗？当然可以，这可以认为是自定义的 hash 算法。

问题是**节点数量变化了怎么办？**简单求取 Hash 值解决了缓存性能的问题，但是没有考虑节点数量变化的场景。假设，移除了其中一台节点，只剩下 9 个，那么之前 `hash(key) % 10` 变成了 `hash(key) % 9`，也就意味着几乎缓存值对应的节点都发生了改变。即几乎所有的缓存值都失效了。节点在接收到对应的请求时，均需要重新去数据源获取数据，容易引起 `缓存雪崩`。

一致性哈希算法将 key 映射到 2^32 的空间中，将这个数字首尾相连，形成一个环。

- 计算**节点/机器(通常使用节点的名称、编号和 IP 地址)的哈希值**，放置在环上。
- 计算 **key 的哈希值，放置在环上，顺时针寻找到的第一个节点，就是应选取的节点/机器**。

一致性哈希算法，在新增/删除节点时，只需要重新定位该节点附近的一小部分数据，而不需要重新定位所有的节点

**数据倾斜问题**

在服务器节点少的时候，容易引起key的倾斜，也就是节点在环上的映射很难的均匀，如果节点都集中中上部分，那么下部分所有的key都会交给一个服务器节点。

为了解决这个问题，引入了虚拟节点的概念，一个真实节点对应多个虚拟节点。

假设 1 个真实节点对应 3 个虚拟节点，那么 peer1 对应的虚拟节点是 peer1-1、 peer1-2、 peer1-3（通常以添加编号的方式实现），其余节点也以相同的方式操作。

- 第一步，计算虚拟节点的 Hash 值，放置在环上。
- 第二步，计算 key 的 Hash 值，在环上顺时针寻找到应选取的虚拟节点，例如是 peer2-1，那么就对应真实节点 peer2。

虚拟节点扩充了节点的数量，解决了节点较少的情况下数据容易倾斜的问题。而且代价非常小，只需要增加一个字典(map)维护真实节点与虚拟节点的映射关系即可。

## CAP

CAP 定理，描述了 **分布式系统** 在 **一致性（C）**、**可用性（A）** 和 **分区容忍性（P）** 三者之间的权衡关系。

+ 在 **分布式系统中，最多只能同时满足 C、A、P 三者中的两个**。

+ 由于网络可能会发生 **分区（Partition）**，因此 **一个分布式系统必须选择在 C 和 A 之间做权衡**。

### **1️⃣ 一致性（Consistency）**

**所有节点上的数据必须保持一致**，即 **任何读操作都必须返回最新的写入数据**。

❌ **代价**：

- **可能降低可用性**（A）：如果某个节点崩溃，可能导致数据不可读。
- **可能增加响应时间**（需要等待多个副本同步）。

### **2️⃣ 可用性（Availability）**

> **每个请求都会获得非错误响应**（不保证最新数据，但保证一定会返回数据）。

✅ **例子**：

- **DNS 系统**：即使部分服务器宕机，仍然可以查询到域名解析结果。
- **NoSQL 数据库（如 Cassandra）**：允许返回旧数据，但保证高可用性。

❌ **代价**：

- **可能降低一致性**（C）：数据可能在某些节点上不同步，导致 **读旧数据（Stale Reads）**。

### **3️⃣ 分区容忍性（Partition Tolerance）**

> **分布式系统在网络分区（Partition）发生时，仍然能够继续运行**。

✅ **例子**：

- **大多数分布式数据库（如 MongoDB、Cassandra）** 允许在部分网络不可达时，仍然可以访问数据。

❌ **代价**：

- **必须在 C（一致性） 和 A（可用性） 之间做取舍**。

**CP（一致性 + 分区容忍）** ➝ 牺牲可用性（A） **Raft**（事务操作强一致性）

**AP（可用性 + 分区容忍）** ➝ 牺牲一致性（C） **DNS**（可能不同服务器返回的数据不一致）





## 分布式系统中的幂等性

比如订单系统在调用的时候，由于网络原因超时了，然后订单系统因为重试机制，重试了，支付系统有收到了两个相同的请求，因为负载均衡分别落在了不同的机器上运行，又出现了重复扣款。

谓幂等性，就是说一个接口，多次发起同一个请求，你这个接口得保证结果是准确得。比如不能多扣款。不能多插入一条数据，不能将统计值多加了1，这就是幂等性。

保证幂等性需要三点：

1. 拥有唯一标识
2. 每次处理完请求后，需要有一个方法标识这个请求已经被处理了
3. 每次接收请求需要进行判断之前是否处理过得逻辑处理

## ZK使用场景

1. 分布式协调：这个其实就是zk很经典的一个用法，简单来说，就好比，你系统A发送个请求到mq，然后B消费了之后处理。那A系统如何指导B系统的处理结果？用zk就可以实现分布式系统之间的协调工作。A系统发送请求之后可以在zk上对某个节点的值注册个监听器，一旦B系统处理完了就修改zk那个节点的值，A立马就可以收到通知，完美解决。
2. 分布所锁：对某一个数据联系发出两个修改操作，两台机器同时收到请求，但是只能一台机器先执行另外一个机器再执行，那么此时就可以使用zk分布式锁，一个机器接收到了请求之后先获取zk上的一把分布式锁，就是可以去创建一个znode，接着执行操作，然后另外一个机器也尝试去创建那个znode，结果发现自己创建不了，因为被别人创建了，那只能等着，等等一个机器执行完了自己再执行。
3. 配置信息管理：zk可以用作很多系统的配置信息的管理，比如kafka，storm等等很多分布式系统都会选用zk来做一些元数据，配置信息的管理，包括dubbo注册中心不也支持zk么

## 分布式锁是什么？redis和zk两种分布式锁的优劣

分布式锁是一种用于 **多个进程或多个节点之间同步访问共享资源** 的机制。它确保在 **分布式环境** 中，**同一时间** 只有一个进程/节点能够访问某个共享资源，避免并发问题，如：

- **任务重复执行**（比如你的分布式渲染任务，一个任务不应该被多个 Worker 计算）
- **数据竞争**（多个进程同时更新数据库，导致数据不一致）
- **资源超卖**（电商秒杀）

### redis中

使用redlock算法

**多个 Redis 实例**（建议至少 5 个）组成 **分布式锁系统**。

**客户端依次向多个 Redis 实例请求加锁**，并为每个锁设置 **相同的超时时间**。

**如果客户端在大多数（N/2+1）个 Redis 实例成功加锁**，则认为获取到分布式锁。

**如果某些 Redis 实例宕机**，Redlock 依然可以正常工作，因为它需要 **多数派（quorum）通过**。

**如果加锁失败**，需要清理掉已经加锁的 Redis 实例，以避免死锁。

> 由于多数规则，因此它最多也只能有一个用户请求到锁



### zk中

Zookeeper 采用 **临时顺序节点**（EPHEMERAL_SEQUENTIAL）+ **ZNode 监听机制** 来实现分布式锁。

**每个客户端** 在 `/lock` 目录下创建一个 **临时顺序节点**：

```
/lock/lock-0001
/lock/lock-0002
/lock/lock-0003
```

**谁创建的序号最小，谁获得锁**（例如 `lock-0001`）。

其他客户端监听前一个节点（比如 `lock-0002` 监听 `lock-0001`）。

**锁释放后**（`lock-0001` 删除），下一个最小节点自动获得锁（`lock-0002`）。

# Zookeeper

Zookeeper也是为了解决分布式一致性提出的算法。`Zookeeper`可以直接和应用程序进行交互，而`Raft`是一个库，它上层还需要运行一个实际的应用程序（比如kv数据库）。

Zookeeper也是一个主从模式的框架，也有leader和follower。它就像设计模式中的观察者模式，Zookeeper负责存储应用程序感兴趣的数据，应用程序会在Zookeeper上注册这些数据，一旦数据发生了变化,Zookeeper就通知那些注册的观察者。

## Zookeeper中的角色和关系

ZooKeeper 中的角色包括 Leader、Follower、Observer。Leader 是集群主节点，主要负责管理集群状态和接收用户的写请求；Follower 是从节点，主要负责集群选举投票和接收用户的读请求；Observer 的功能与 Follower 类似，只是没有投票权，主要用于分担 Follower 的读请求，降低集群的负载。

a. Leader

一个运行中的 Zookeeper 集群只有一个 Leader 服务，Leader 服务主要包括以下两个指责：

- 负责集群数据的写操作。所有写操作必须要 Leader 完成之后，才可以将写操作广播到其他 Follower，并且只有**超过半数节点（不包括 Observer）写入成功**后，这些写请求才算写成功；
- 发起并**维护各个 Follower 以及 Observer 之间的心跳**，以监控集群的运行状态。

b. Follower
一个 Zookeeper 集群可以有多个 Follower，Follower通过心跳与 Leader 保持连接。Follower 服务主要有以下两个指责：

- 负责集群数据的读操作。Follower 在接受到一个客户端请求之后，会**先判断该请求是读请求还是写请求，若为读请求，则 Follower 从本地节点上读取数据并返回给客户端；若为写请求，则 Follower 会将写请求转发给 Leader 来处理。**(这里和Raft不一样，Raft就算是读也会交给leader处理，为了保证强一致性。)
- 参与集群中 Leader 的选举。当 Leader 失效之后，Follower 需要在集群**选举**时进行投票；（后续会详细讲解选举机制）

c. Observer

一个 Zookeeper 集群可以有多个 Observer，Observer 的主要职责是负责集群数据的读操作，其功能同以上介绍的 Follower 的功能类似，主要的差别就是 Observer 没有投票权。

> Observor角色的引入是因为，对于一个Zookeeper集群如果要支持更多的服务器并发操作，就需要更多的服务实例，但是过多的服务实例会让投票变得很复杂，选举时间也会变长不利于快速回复。

Zookeeper运行在ZAB之上的，ZAB就类似之前Raft属于一个层级，在底层维护一系列log。对于Zookeeper它提升了读请求的效率：

1. `raft`的效率
   由于我们目前已经实现了`raft`协议, 因此我们知道, 服务的性能并不随服务器数量的提升线性增长, 甚至会有性能下降, 因为`raft`需要将一个日志复制到过半的节点后才能标记为`commit`, 因此服务器越多, 这一步骤耗费越大, 甚至拖累整个几集群的运行速度。
2. `ZooKeeper`的效率
   `ZooKeeper`的读请求直接发送给一个随机的副本处理, 不需要在真个集群内进行同步, 因此其运行速度更快, 缺点是**除了`Leader`以外的任何一个副本节点的数据是不一定是最新的**



## ZAB

ZAB是专门给Zookeeper设计的保证Zookeeper服务一致性的算法，**它是支持崩溃恢复的原子广播协议**。之前已经说过Zookeeper是主从模型，也就是有leader与follwer，而它们之前的一致性就要靠ZAB来保证——Zookeeper只有leader节点负责处理外部的写事务。*也就是说，客户端随机连接到Zookeeper中的一个节点，如果是读请求就会自己从本地节点读取；如果是写请求，那么节点就会向 Leader 提交事务，Leader 接收到事务提交，会广播该事务，只要超过半数节点写入成功，该事务就会被提交。*

ZAB有两大部分 **消息广播和崩溃恢复**

#### 消息广播

1. Leader 接收到消息请求后，将消息赋予一个全局唯一的 64 位自增 id，叫做：zxid，通过 zxid 的大小比较即可实现因果有序这一特性。
2. Leader 通过先进先出队列（通过 TCP 协议来实现，以此实现了全局有序这一特性）将带有 zxid 的消息作为一个提案（proposal）分发给所有 follower。
3. 当 follower 接收到 proposal，先将 proposal 写到硬盘，写硬盘成功后再向 leader 回一个 ACK。
4. 当 leader 接收到合法数量的 ACKs 后，leader 就向所有 follower 发送 COMMIT 命令，同时会在本地执行该消息。
5. 当 follower 收到消息的 COMMIT 命令时，就会执行该消息。

#### 崩溃恢复

对于ZAB，在他崩溃恢复起很大作用的参数有下面：

+ myid: 服务器ID，myid越大优先级越高，主要是为了规避选举的时候如果有多个节点都有相同最大zxid，那么投票不知道怎么投的问题。这就像Raft中使用随机选举定时器时间，避免多个节点同时开启选举，导致选不出来leader。
+ Zxid: 对于事务的标号，注意ZAB中只关心写事务。这个zxid是一个64位的，高32位是epoch(其实就是和Term差不多的概念，每次选出一个新的leader, epoch+1)，低32位就是这个写事务的编号。zxid越大，表示当前节点上提交成功了最新的事务。
+ Epoch: 可以理解为Term

选举的状态如下：
```
LOOKING: 竞选状态。
FOLLOWING: 随从状态，同步Leader 状态，参与Leader选举的投票过程。
OBSERVING: 观察状态，同步Leader 状态，不参与Leader选举的投票过程。
LEADING: 领导者状态。
```

选举的流程如下：

- 每个Server会发出一个投票,第一次都是投自己。投票信息：（myid，ZXID）
- 收集来自各个服务器的投票
- 处理投票并重新投票，处理逻辑：优先比较ZXID,然后比较myid
- 统计投票，只要超过半数的机器接收到同样的投票信息，就可以确定leader
- 改变服务器状态，进入正常的消息广播流程。

ZAB算法处理下面两个问题：

> 当 leader 收到合法数量 follower 的 ACKs 后，就向各个 follower 广播 COMMIT 命令，同时也会在本地执行 COMMIT 并向连接的客户端返回「成功」。但是如果在各个 follower 在收到 COMMIT 命令前 leader 就挂了，导致剩下的服务器并没有执行都这条消息。

1. ZAB的选举机制会保证拥有最大zxid的节点当选新的leader
2. 新的 leader 将自己事务日志中 proposal 但未 COMMIT 的消息处理。
3. 新的 leader 与 follower 建立先进先出的队列， 先将自身有而 follower 没有的 proposal 发送给 follower，再将这些 proposal 的 COMMIT 命令发送给 follower，以保证所有的 follower 都保存了所有的 proposal、所有的 follower 都处理了所有的消息。

> 当 leader 接收到消息请求生成 proposal 后就挂了，其他 follower 并没有收到此 proposal，因此经过恢复模式重新选了 leader 后，这条消息是被跳过的。 此时，之前挂了的 leader 重新启动并注册成了 follower，他保留了被跳过消息的 proposal 状态，与整个系统的状态是不一致的，需要将其删除。

zxid的设计是高32位是epoch,每次新选举都会把高32位+1，低32位清零。老的leader重新加入后，不会被重新选为leader因为它的zxid不可能更大。当旧的 leader 作为 follower 接入新的 leader 后，新的 leader 会让它将所有的拥有旧的 epoch 号的未被 COMMIT 的 proposal 清除。

**ZAB 的日志同步和 Raft 的日志同步有何不同？**
 ✅ **Raft 采用 "日志强制匹配" 机制（Log Matching Property），如果 Follower 的日志不匹配，Leader 直接覆盖它的日志。而 ZAB 采用 "日志截断" 机制，Follower 需要主动回滚。**

----



所以Zookeeper放弃了读操作的线性一致性。

Zookeeper的结构是一个树，它有点类似文件系统：

![ZooKeeper-namespace](https://obsdian-1304266993.cos.ap-chongqing.myqcloud.com/202503141048952.png)

每一个节点都是叫znode,Zookeeper就是通过对这些节点CRUD操作来实现各种分布式服务。Zookeeper对这些操作有如下的保证:

1. 写操作的线性一致性 (`Linearizable Writes`)
   ZooKeeper保证所有更新其状态的请求都是可序列化的，并且遵循先行顺序。这意味着写操作是原子的，并且系统中的所有其他操作都将看到这一操作之前或之后的状态，而不会有部分更新的现象。这是分布式系统中对于一致性的一个关键保证。

2. 客户端FIFO顺序 (`FIFO Client Order`)
   来自同一个客户端的所有请求都将按照它们被客户端发送的顺序执行。这意味着同一个客户端发起的操作将会按照其发起的顺序被严格处理，保证了客户端视图的顺序性。

3. 线性一致性 (`A-linearizability`)
   `ZooKeeper`定义的线性一致性称为异步线性一致性（`A-linearizability`）。在Herlihy的定义中，客户端一次只能有一个操作在执行中（如同一个线程）。而在`ZooKeeper`的定义中，一个客户端可以有多个未完成的操作，因此可以选择不保证同一客户端的未完成操作的特定顺序，或者保证FIFO顺序。`ZooKeeper`选择了后者。

4. 读操作的本地处理
   由于只有更新请求是`A-linearizable`，`ZooKeeper`将读请求在每个副本上本地处理。这允许服务随着服务器的增加而线性扩展。

5. 交互的保证
   举例来说，如果一个系统中的多个进程选举出一个`Leader`来指挥工作进程，当一个新的`Leader`接管系统时，它必须更改许多配置参数，并在完成后通知其他进程。在这种情况下，`ZooKeeper`需要保证：

- 当新`Leader`开始进行更改时，不希望其他进程开始使用正在更改的配置；
- 如果新`Leader`在配置完全更新之前死亡，不希望进程使用这个部分配置。

6. 存活性和持久性保证
   `ZooKeeper`还保证了存活性和持久性：

- 只要`ZooKeeper`服务器的大多数都是活跃的并且能够通信，服务就会可用；
- 如果`ZooKeeper`服务成功响应了一个变更请求，那么只要服务器的法定人数最终能够恢复，该变更就会持续存在，不会因为任何数量的故障而丢失。

7. 同步读 (`sync`)
   为了处理由于客户端之间通信延迟导致的潜在问题，`ZooKeeper`提供了`sync`请求。如果在执行读操作之前使用`sync`，它会导致服务器先应用所有挂起的写请求再处理读请求，这样客户端就能看到最新的状态，而不需要承担完整写操作的开销。

![image-20250314160934132](https://obsdian-1304266993.cos.ap-chongqing.myqcloud.com/202503141609197.png)

## Zookeeper服务案例

### 分布式锁

# MapReduce

MapReduce是谷歌提出来的一个框架，它的思想是，应用程序的设计者只需要编写Map与Reduce函数，而不需要知道有关分布式的任何事情，MapReduce框架处理剩下的事情。

首先MapReduce从集群中选出一个机器当Master,负责分配任务、监督进度、错误处理。然后其他的机器当作是worker。先是给所有的节点安排Map任务，Map任务执行完后再分配Reduce任务。

假设输入的数据已经分片，Master会协调把这些分片分配给worker，但是worker执行Map任务，输出一个中间结果。（输入是kv数据，输出是kv列表，比如说统计单词数量的话，输入是一段文字，输出就是类似某个单词出现的次数这样的kv对）。得到的中间结果还要进行分片，就是对中间结果的键求哈希然后按Reduce任务的数量取余。这样的话同一个key都是由同一个Reduce任务处理。在一个分区中，所有中间k/v都是按key排序的。

所谓map任务，一般是把原始的数据（比如文本，日志）进行初步的数据处理，或者是提取，可以转换为key-value对的中间输出。

reduce任务就是执行聚合计算。

> 可以想一下，如果我们不让一个key被固定的reduce节点处理会发生什么。如果节点1有某个key，节点2也有这个key,为了能正确进行reduce操作，就必须额外进行网络通信来使得一个节点得到完整的kv信息。但是如果我们早早的在map阶段就决定了某个key交给固定的reduce节点来执行，那么就不需要额外的网络通信。

- 心跳信号
  `Worker`只需要向`Master`发送心跳信号表示自身的存活, 如果Master在预定时间内没有收到来自某个`Worker`的心跳，它就会将该`Worker`标记为失效，并将其上运行的所有`Map`和`Reduce`任务重新调度到其他节点上。不过这种设计不太会再`lab 1`中出现, 因为这样会使`Master`记录太多有关`Task`和`Worker`的信息, 设计相对复杂
- 超时重试
  如果一个`Worker`节点在执行`Map`或`Reduce`任务耗时过长，`Master`会检测到这种情况。`Master`将其认定为失败, 可以将失败的任务重新分配给其他健康的`Worker`节点执行。这种重试机制可以处理机器故障、软件错误或其他导致任务失败的问题。
- `checkpoints`
  `Master`会周期性地写入`checkpoints`到磁盘以预备可能的崩溃恢复
- 原子重命名
  将`Map`和`Reduce`任务的输出写入到一个命名好的临时文件，并且只在任务成功完成时才对其进行重命名，来实现任务的幂等性。
- 文件系统备份
  在`MapReduce`框架中，输入数据通常存储在一个分布式文件系统（如`GFS`）中，该文件系统会将数据块复制到多个节点上。这种数据副本机制确保了即使某些机器发生故障，数据仍然可用。

# 分布式共识算法

下面几个算法都是为了保证分布式一致性提出的算法。所谓分布式一致性一般是指的数据一致性，比如分布式存储中，一个数据有多个副本，且这些副本在不同的节点上，需要用一致性算法来保障从不同节点的数据副本中取得值是相同的。



## Paxos

和Raft一样都有多数投票的机制，所以都可以容忍「(N-1)/2」个节点故障。在Paxos原论文中，没有像Raft一样清楚的给出了内置的选举机制是如何的，也没说完整的日志同步与提交规则。Raft中日志顺序是严格一致的。

## Raft



## ZAB
