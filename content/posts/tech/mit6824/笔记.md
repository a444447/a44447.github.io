---
title: "笔记"
date: 2025-01-23T15:30:47+08:00
categories: [技术,mit6824]
---

这是在学习mit6824课程中的笔记

# Lecture1

本章是介绍

首先提到了建立分布式架构的几个原因:

+ 需要并行的场景
+ 容错
+ physical
+ 安全性

对于parallelism与fault tolerant是本课程的重点。

接下来说明了为什么分布式系统很难实现：



---

课程主要讨论基础架构: 存粗、通信、计算。**其中存储是重点关注的**。



### mapreduce的基本工作方式

# lecture2

为什么在本课程中使用go语言

+ 简单
+ 垃圾回收且不需要像c++一样思考在什么时候回收线程的内存
+ 提供了易用的库

在编写分布式系统的时候，线程是关注的重点。这在go中称为「go routines」协程。

# lecture3

lecture3主要讨论的是GFS，「the google file system」。这门课的主要内容也是大型存粗。在分布式系统中，简单的存储接口往往十分有用并且很通用。所有构建分布式系统很多时候都是有关如何设计一个存储系统。

为什么分布式存储如何困难？

```
设计一个分布式系统的通常出发点是获得性能提升，所以我们想要的是「数百台计算机资源来同时完成大量工作」，所以我们会自然的想把数据分割到大量的服务器上，这样就可以并行的从多台服务器上读取数据——称为「切片」

但是一旦数千台服务器中某一个服务器宕机，我们需要自动化的能处理修复错误，因为我们就需要自动的容错(falut tolerance)

实现容错最有用的一种方法是使用复制，只需要维护2-3个数据的副本，当其中一个故障了，你就可以使用另一个。所以，如果想要容错能力，就得有复制（replication）。

有了复制，我们就拥有了多份数据的副本，一旦不小心就会不一致，于是我们就需要额外的操作来解决不一致的问题（inconsistency）。

但是为了解决一致性而提出的算法往往会导致我们额外的工作，比如额外的网络通信。这些操作又会导致低性能，这与我们一开始的要求违背了。
```

> 弱一致性：不一定强制所有副本都是同步的，而是有一定的容忍度
>
> 强一致性：保证每个副本都是要同步的

## GFS机制

GFS存在一系列缺点，但是是第一个将分布式架构引入到大型业务中的设计。

它的设计目的是设计一个大型的文件系统，**不同的应用程序都可以从中读取数据**，这是一个通用的存储系统。

GFS的一些特点：

1. 为了拥有快速和高容量的特性，会将文件切割存放在多个服务器上，这样就可以从多个服务器中读取同一个文件。
2. 需要有自动的故障恢复
3. GFS只在一个数据中心运行，而没有将副本存储在世界各地
4. GFS是对大型的顺序文件做的定制，为了TB级别的文件而生存，它只支持顺序处理而不支持随机访问。

### Master

在GFS中分了两种服务器：

+ Master节点：管理文件和Chunk信息
+ Chunk服务器：存储实际的数据

Master节点保存了两个表单:

1. 文件名到Chunk ID数组的映射。这个表单告诉我们文件对应了哪些chunk
2. 光只有第一个表是没用的，我们需要第二个表单，提供了这些信息：

- 每个Chunk存储在哪些服务器上，所以这部分是Chunk服务器的列表
- 每个Chunk当前的版本号，所以Master节点必须记住每个Chunk对应的版本号。
- 所有对于Chunk的写操作都必须在主Chunk（Primary Chunk）上顺序处理，主Chunk是Chunk的多个副本之一。所以，Master节点必须记住哪个Chunk服务器持有主Chunk。
- 并且，主Chunk只能在特定的租约时间内担任主Chunk，所以，Master节点要记住主Chunk的租约过期时间。

以上数据都是 **存储在内存**的，一旦masrer故障，数据就丢失了。为了能让Master重启并且不丢失数据，Master节点会同时将数据存储在磁盘上（部分内容）。总结就是，Master节点读数据都是从内存读，但是写数据会把部分数据写入磁盘。

Master存储磁盘的方式是存储log，每次有数据变更就会在磁盘的log中追加一个记录。

> 每次master重启时，并不会从头开始读取log，而是会从特定的checkpoint开始恢复，从该checkpoint后面开始读取log逐步恢复。

**注意，一个chunk默认的大小是64mb**



### 读机制

对于读操作，首先就是某个应用程序或者说GFS客户端有一个文件名和它想从文件的某个位置读取的offset，这样的请求会发送给master节点。

master节点从*file表单*中根据文件名查询到chunk id数组，通过(offset / 64mb)就可以从数组中找到对应的chunk id。之后从*chunk表单*中找到存有对应chunk的服务器列表，这个列表返回给客户端。

然后客户端根据这个列表，选择最优副本进行读取（比如离自己最近的副本，减少网络延迟）如果该副本不可用再换另外的。*数据块通常会按顺序传输至客户端，客户端会将其拼接起来以组成完整的文件。*



### 写机制

对于写文件，应用程序调用GFS库，向master发送请求说：我想向这个文件名对应的文件追加数据，请告诉我文件中**最后一个Chunk的位置**。

由于当有 **多个客户端**同时写文件时，一个客户端并不知道文件究竟有多长，因为它无法知道其他客户端写了多少。为了避免，客户端应该向master节点查询哪个chunk服务器保存了文件的最后一个chunk.

读文件可以从任何最新的副本读，但是写的话只能写入主副本(primary chunk)。

> 对于某个特定的Chunk来说，在某一个时间点，Master不一定指定了Chunk的主副本。所以，写文件的时候，需要考虑Chunk的主副本不存在的情况.
>
> 对于Master节点来说，如果发现Chunk的主副本不存在，Master会找出所有存有Chunk最新副本的Chunk服务器。如果你的一个系统已经运行了很长时间，那么有可能某一个Chunk服务器保存的Chunk副本是旧的，比如说还是昨天或者上周的。导致这个现象的原因可能是服务器因为宕机而没有收到任何的更新。所以，Master节点需要能够在Chunk的多个副本中识别出，哪些副本是新的，哪些是旧的。所以第一步是，找出新的Chunk副本。这一切都是在Master节点发生，因为，现在是客户端告诉Master节点说要追加某个文件，Master节点需要告诉客户端向哪个Chunk服务器（也就是Primary Chunk所在的服务器）去做追加操作。所以，Master节点的部分工作就是弄清楚在追加文件时，客户端应该与哪个Chunk服务器通信。
>
> 最新的副本是指，副本中保存的版本号与Master中记录的**Chunk的版本号一致**。Chunk副本中的版本号是由Master节点下发的，所以Master节点知道，对于一个特定的Chunk，哪个版本号是最新的。

当客户端想要对文件进行追加，但是又不知道文件尾的Chunk对应的Primary在哪时，Master会等所有存储了最新Chunk版本的服务器集合完成，然后挑选一个作为Primary，其他的作为Secondary。之后master节点增加版本号并写入磁盘，这样就算故障了也不会丢失。

> 接下来，Master节点会向Primary和Secondary副本对应的服务器发送消息并告诉它们，谁是Primary，谁是Secondary，Chunk的新版本是什么。Primary和Secondary服务器都会将版本号存储在本地的磁盘中。这样，当它们因为电源故障或者其他原因重启时，它们可以向Master报告本地保存的Chunk的实际版本号。

**所以，现在我们有了一个primary，它可以接受来自客户端的写请求**，并将写请求应用在多个Chunk服务器中。之所以要管理Chunk的版本号，是因为这样Master可以将实际更新Chunk的能力转移给Primary服务器。并且在将版本号更新到Primary和Secondary服务器之后，如果Master节点故障重启，还是可以在相同的Primary和Secondary服务器上继续更新Chunk。

现在，Master节点通知Primary和Secondary服务器，你们可以修改这个Chunk。它还给Primary一个租约，这个租约告诉Primary说，在接下来的60秒中，你将是Primary，60秒之后你必须停止成为Primary。这种机制可以确保我们不会同时有两个Primary。



所以现在master节点告诉了我们谁是primary谁是secondary，gfs对于写操作的执行序列是这样的：

> 客户端会将要追加的数据发送给Primary和Secondary服务器，这些服务器会将数据写入到一个临时位置。所以最开始，这些数据不会追加到文件中。当所有的服务器都返回确认消息说，已经有了要追加的数据，客户端会向Primary服务器发送一条消息说，你和所有的Secondary服务器都有了要追加的数据，现在我想将这个数据追加到这个文件中。Primary服务器或许会从大量客户端收到大量的并发请求，Primary服务器会以某种顺序，一次只执行一个请求。对于每个客户端的追加数据请求（也就是写请求），Primary会查看当前文件结尾的Chunk，并确保Chunk中有足够的剩余空间，然后将客户端要追加的数据写入Chunk的末尾。并且，Primary会通知所有的Secondary服务器也将客户端要追加的数据写入在它们自己存储的Chunk末尾。这样，包括Primary在内的所有副本，都会收到通知将数据追加在Chunk的末尾。

primary通知secondary服务器追加数据，有可能这些secondary服务器执行成功也可能执行失败，比如磁盘空间不够，比如故障，比如丢包。如果至少有一个secondary服务器没有回复primary "yes:,那么primary向客户端返回写入失败。

---

GFS最严重的局限可能在于，它只有一个Master节点，会带来以下问题：

- Master节点必须为每个文件，每个Chunk维护表单，随着GFS的应用越来越多，这意味着涉及的文件也越来越多，最终Master会耗尽内存来存储文件表单。你可以增加内存，但是单台计算机的内存也是有上限的。所以，这是人们遇到的最早的问题。
- 除此之外，单个Master节点要承载数千个客户端的请求，而Master节点的CPU每秒只能处理数百个请求，尤其Master还需要将部分数据写入磁盘，很快，客户端数量超过了单个Master的能力。
- 另一个问题是，应用程序发现很难处理GFS奇怪的语义（本节最开始介绍的GFS的副本数据的同步，或者可以说不同步）。
- 最后一个问题是，从我们读到的GFS论文中，Master节点的故障切换不是自动的。GFS需要人工干预来处理已经永久故障的Master节点，并更换新的服务器，这可能需要几十分钟甚至更长的而时间来处理。对于某些应用程序来说，这个时间太长了。
