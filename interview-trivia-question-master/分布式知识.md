[分布式相关面试题先来看看简单的分布式系统机架构图 初步了解一下分布式系统是个什么玩意 为什么要将系统进行拆分 要是不拆分 - 掘金](https://juejin.cn/post/6996915693811662884)


## 分布式系统中的幂等性

# Zookeeper

Zookeeper也是为了解决分布式一致性提出的算法。`Zookeeper`可以直接和应用程序进行交互，而`Raft`是一个库，它上层还需要运行一个实际的应用程序（比如kv数据库）。

Zookeeper也是一个主从模式的框架，也有leader和follower。它就像设计模式中的观察者模式，Zookeeper负责存储应用程序感兴趣的数据，应用程序会在Zookeeper上注册这些数据，一旦数据发生了变化,Zookeeper就通知那些注册的观察者。

## Zookeeper中的角色和关系

ZooKeeper 中的角色包括 Leader、Follower、Observer。Leader 是集群主节点，主要负责管理集群状态和接收用户的写请求；Follower 是从节点，主要负责集群选举投票和接收用户的读请求；Observer 的功能与 Follower 类似，只是没有投票权，主要用于分担 Follower 的读请求，降低集群的负载。

a. Leader

一个运行中的 Zookeeper 集群只有一个 Leader 服务，Leader 服务主要包括以下两个指责：

- 负责集群数据的写操作。所有写操作必须要 Leader 完成之后，才可以将写操作广播到其他 Follower，并且只有**超过半数节点（不包括 Observer）写入成功**后，这些写请求才算写成功；
- 发起并**维护各个 Follower 以及 Observer 之间的心跳**，以监控集群的运行状态。

b. Follower
一个 Zookeeper 集群可以有多个 Follower，Follower通过心跳与 Leader 保持连接。Follower 服务主要有以下两个指责：

- 负责集群数据的读操作。Follower 在接受到一个客户端请求之后，会**先判断该请求是读请求还是写请求，若为读请求，则 Follower 从本地节点上读取数据并返回给客户端；若为写请求，则 Follower 会将写请求转发给 Leader 来处理。**(这里和Raft不一样，Raft就算是读也会交给leader处理，为了保证强一致性。)
- 参与集群中 Leader 的选举。当 Leader 失效之后，Follower 需要在集群**选举**时进行投票；（后续会详细讲解选举机制）

c. Observer

一个 Zookeeper 集群可以有多个 Observer，Observer 的主要职责是负责集群数据的读操作，其功能同以上介绍的 Follower 的功能类似，主要的差别就是 Observer 没有投票权。

> Observor角色的引入是因为，对于一个Zookeeper集群如果要支持更多的服务器并发操作，就需要更多的服务实例，但是过多的服务实例会让投票变得很复杂，选举时间也会变长不利于快速回复。

Zookeeper运行在ZAB之上的，ZAB就类似之前Raft属于一个层级，在底层维护一系列log。对于Zookeeper它提升了读请求的效率：

1. `raft`的效率
   由于我们目前已经实现了`raft`协议, 因此我们知道, 服务的性能并不随服务器数量的提升线性增长, 甚至会有性能下降, 因为`raft`需要将一个日志复制到过半的节点后才能标记为`commit`, 因此服务器越多, 这一步骤耗费越大, 甚至拖累整个几集群的运行速度。
2. `ZooKeeper`的效率
   `ZooKeeper`的读请求直接发送给一个随机的副本处理, 不需要在真个集群内进行同步, 因此其运行速度更快, 缺点是**除了`Leader`以外的任何一个副本节点的数据是不一定是最新的**



## ZAB

ZAB是专门给Zookeeper设计的保证Zookeeper服务一致性的算法，**它是支持崩溃恢复的原子广播协议**。之前已经说过Zookeeper是主从模型，也就是有leader与follwer，而它们之前的一致性就要靠ZAB来保证——Zookeeper只有leader节点负责处理外部的写事务。*也就是说，客户端随机连接到Zookeeper中的一个节点，如果是读请求就会自己从本地节点读取；如果是写请求，那么节点就会向 Leader 提交事务，Leader 接收到事务提交，会广播该事务，只要超过半数节点写入成功，该事务就会被提交。*

ZAB有两大部分 **消息广播和崩溃恢复**

#### 消息广播

1. Leader 接收到消息请求后，将消息赋予一个全局唯一的 64 位自增 id，叫做：zxid，通过 zxid 的大小比较即可实现因果有序这一特性。
2. Leader 通过先进先出队列（通过 TCP 协议来实现，以此实现了全局有序这一特性）将带有 zxid 的消息作为一个提案（proposal）分发给所有 follower。
3. 当 follower 接收到 proposal，先将 proposal 写到硬盘，写硬盘成功后再向 leader 回一个 ACK。
4. 当 leader 接收到合法数量的 ACKs 后，leader 就向所有 follower 发送 COMMIT 命令，同时会在本地执行该消息。
5. 当 follower 收到消息的 COMMIT 命令时，就会执行该消息。

#### 崩溃恢复

对于ZAB，在他崩溃恢复起很大作用的参数有下面：

+ myid: 服务器ID，myid越大优先级越高，主要是为了规避选举的时候如果有多个节点都有相同最大zxid，那么投票不知道怎么投的问题。这就像Raft中使用随机选举定时器时间，避免多个节点同时开启选举，导致选不出来leader。
+ Zxid: 对于事务的标号，注意ZAB中只关心写事务。这个zxid是一个64位的，高32位是epoch(其实就是和Term差不多的概念，每次选出一个新的leader, epoch+1)，低32位就是这个写事务的编号。zxid越大，表示当前节点上提交成功了最新的事务。
+ Epoch: 可以理解为Term

选举的状态如下：
```
LOOKING: 竞选状态。
FOLLOWING: 随从状态，同步Leader 状态，参与Leader选举的投票过程。
OBSERVING: 观察状态，同步Leader 状态，不参与Leader选举的投票过程。
LEADING: 领导者状态。
```

选举的流程如下：

- 每个Server会发出一个投票,第一次都是投自己。投票信息：（myid，ZXID）
- 收集来自各个服务器的投票
- 处理投票并重新投票，处理逻辑：优先比较ZXID,然后比较myid
- 统计投票，只要超过半数的机器接收到同样的投票信息，就可以确定leader
- 改变服务器状态，进入正常的消息广播流程。

ZAB算法处理下面两个问题：

> 当 leader 收到合法数量 follower 的 ACKs 后，就向各个 follower 广播 COMMIT 命令，同时也会在本地执行 COMMIT 并向连接的客户端返回「成功」。但是如果在各个 follower 在收到 COMMIT 命令前 leader 就挂了，导致剩下的服务器并没有执行都这条消息。

1. ZAB的选举机制会保证拥有最大zxid的节点当选新的leader
2. 新的 leader 将自己事务日志中 proposal 但未 COMMIT 的消息处理。
3. 新的 leader 与 follower 建立先进先出的队列， 先将自身有而 follower 没有的 proposal 发送给 follower，再将这些 proposal 的 COMMIT 命令发送给 follower，以保证所有的 follower 都保存了所有的 proposal、所有的 follower 都处理了所有的消息。

> 当 leader 接收到消息请求生成 proposal 后就挂了，其他 follower 并没有收到此 proposal，因此经过恢复模式重新选了 leader 后，这条消息是被跳过的。 此时，之前挂了的 leader 重新启动并注册成了 follower，他保留了被跳过消息的 proposal 状态，与整个系统的状态是不一致的，需要将其删除。

zxid的设计是高32位是epoch,每次新选举都会把高32位+1，低32位清零。老的leader重新加入后，不会被重新选为leader因为它的zxid不可能更大。当旧的 leader 作为 follower 接入新的 leader 后，新的 leader 会让它将所有的拥有旧的 epoch 号的未被 COMMIT 的 proposal 清除。

**ZAB 的日志同步和 Raft 的日志同步有何不同？**
 ✅ **Raft 采用 "日志强制匹配" 机制（Log Matching Property），如果 Follower 的日志不匹配，Leader 直接覆盖它的日志。而 ZAB 采用 "日志截断" 机制，Follower 需要主动回滚。**

----



所以Zookeeper放弃了读操作的线性一致性。

Zookeeper的结构是一个树，它有点类似文件系统：

![ZooKeeper-namespace](https://obsdian-1304266993.cos.ap-chongqing.myqcloud.com/202503141048952.png)

每一个节点都是叫znode,Zookeeper就是通过对这些节点CRUD操作来实现各种分布式服务。Zookeeper对这些操作有如下的保证:

1. 写操作的线性一致性 (`Linearizable Writes`)
   ZooKeeper保证所有更新其状态的请求都是可序列化的，并且遵循先行顺序。这意味着写操作是原子的，并且系统中的所有其他操作都将看到这一操作之前或之后的状态，而不会有部分更新的现象。这是分布式系统中对于一致性的一个关键保证。

2. 客户端FIFO顺序 (`FIFO Client Order`)
   来自同一个客户端的所有请求都将按照它们被客户端发送的顺序执行。这意味着同一个客户端发起的操作将会按照其发起的顺序被严格处理，保证了客户端视图的顺序性。

3. 线性一致性 (`A-linearizability`)
   `ZooKeeper`定义的线性一致性称为异步线性一致性（`A-linearizability`）。在Herlihy的定义中，客户端一次只能有一个操作在执行中（如同一个线程）。而在`ZooKeeper`的定义中，一个客户端可以有多个未完成的操作，因此可以选择不保证同一客户端的未完成操作的特定顺序，或者保证FIFO顺序。`ZooKeeper`选择了后者。

4. 读操作的本地处理
   由于只有更新请求是`A-linearizable`，`ZooKeeper`将读请求在每个副本上本地处理。这允许服务随着服务器的增加而线性扩展。

5. 交互的保证
   举例来说，如果一个系统中的多个进程选举出一个`Leader`来指挥工作进程，当一个新的`Leader`接管系统时，它必须更改许多配置参数，并在完成后通知其他进程。在这种情况下，`ZooKeeper`需要保证：

- 当新`Leader`开始进行更改时，不希望其他进程开始使用正在更改的配置；
- 如果新`Leader`在配置完全更新之前死亡，不希望进程使用这个部分配置。

6. 存活性和持久性保证
   `ZooKeeper`还保证了存活性和持久性：

- 只要`ZooKeeper`服务器的大多数都是活跃的并且能够通信，服务就会可用；
- 如果`ZooKeeper`服务成功响应了一个变更请求，那么只要服务器的法定人数最终能够恢复，该变更就会持续存在，不会因为任何数量的故障而丢失。

7. 同步读 (`sync`)
   为了处理由于客户端之间通信延迟导致的潜在问题，`ZooKeeper`提供了`sync`请求。如果在执行读操作之前使用`sync`，它会导致服务器先应用所有挂起的写请求再处理读请求，这样客户端就能看到最新的状态，而不需要承担完整写操作的开销。

![image-20250314160934132](https://obsdian-1304266993.cos.ap-chongqing.myqcloud.com/202503141609197.png)

## Zookeeper服务案例

### 分布式锁

# MapReduce

MapReduce是谷歌提出来的一个框架，它的思想是，应用程序的设计者只需要编写Map与Reduce函数，而不需要知道有关分布式的任何事情，MapReduce框架处理剩下的事情。

首先MapReduce从集群中选出一个机器当Master,负责分配任务、监督进度、错误处理。然后其他的机器当作是worker。先是给所有的节点安排Map任务，Map任务执行完后再分配Reduce任务。

假设输入的数据已经分片，Master会协调把这些分片分配给worker，但是worker执行Map任务，输出一个中间结果。（输入是kv数据，输出是kv列表，比如说统计单词数量的话，输入是一段文字，输出就是类似某个单词出现的次数这样的kv对）。得到的中间结果还要进行分片，就是对中间结果的键求哈希然后按Reduce任务的数量取余。这样的话同一个key都是由同一个Reduce任务处理。在一个分区中，所有中间k/v都是按key排序的。

所谓map任务，一般是把原始的数据（比如文本，日志）进行初步的数据处理，或者是提取，可以转换为key-value对的中间输出。

reduce任务就是执行聚合计算。

> 可以想一下，如果我们不让一个key被固定的reduce节点处理会发生什么。如果节点1有某个key，节点2也有这个key,为了能正确进行reduce操作，就必须额外进行网络通信来使得一个节点得到完整的kv信息。但是如果我们早早的在map阶段就决定了某个key交给固定的reduce节点来执行，那么就不需要额外的网络通信。

- 心跳信号
  `Worker`只需要向`Master`发送心跳信号表示自身的存活, 如果Master在预定时间内没有收到来自某个`Worker`的心跳，它就会将该`Worker`标记为失效，并将其上运行的所有`Map`和`Reduce`任务重新调度到其他节点上。不过这种设计不太会再`lab 1`中出现, 因为这样会使`Master`记录太多有关`Task`和`Worker`的信息, 设计相对复杂
- 超时重试
  如果一个`Worker`节点在执行`Map`或`Reduce`任务耗时过长，`Master`会检测到这种情况。`Master`将其认定为失败, 可以将失败的任务重新分配给其他健康的`Worker`节点执行。这种重试机制可以处理机器故障、软件错误或其他导致任务失败的问题。
- `checkpoints`
  `Master`会周期性地写入`checkpoints`到磁盘以预备可能的崩溃恢复
- 原子重命名
  将`Map`和`Reduce`任务的输出写入到一个命名好的临时文件，并且只在任务成功完成时才对其进行重命名，来实现任务的幂等性。
- 文件系统备份
  在`MapReduce`框架中，输入数据通常存储在一个分布式文件系统（如`GFS`）中，该文件系统会将数据块复制到多个节点上。这种数据副本机制确保了即使某些机器发生故障，数据仍然可用。

# 分布式共识算法

下面几个算法都是为了保证分布式一致性提出的算法。所谓分布式一致性一般是指的数据一致性，比如分布式存储中，一个数据有多个副本，且这些副本在不同的节点上，需要用一致性算法来保障从不同节点的数据副本中取得值是相同的。



## Paxos

和Raft一样都有多数投票的机制，所以都可以容忍「(N-1)/2」个节点故障。在Paxos原论文中，没有像Raft一样清楚的给出了内置的选举机制是如何的，也没说完整的日志同步与提交规则。Raft中日志顺序是严格一致的。

## Raft



## ZAB
