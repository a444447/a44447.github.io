# IDL编译前端



## idl_compiler

![image-20250903153235881](assets/image-20250903153235881.png)

![image-20250903153301286](assets/image-20250903153301286.png)

![image-20250903153339043](assets/image-20250903153339043.png)

![image-20250903153352582](assets/image-20250903153352582.png)

## idl_lex

![image-20250903153849912](assets/image-20250903153849912.png)

![image-20250903153937002](assets/image-20250903153937002.png)

![image-20250903154005100](assets/image-20250903154005100.png)

![image-20250903154029231](assets/image-20250903154029231.png)

![image-20250903154124230](assets/image-20250903154124230.png)

![image-20250903154216207](assets/image-20250903154216207.png)

![image-20250903154233115](assets/image-20250903154233115.png)

## idl_ir_generator

![image-20250903155131480](assets/image-20250903155131480.png)

![image-20250903155309079](assets/image-20250903155309079.png)

![image-20250903155352154](assets/image-20250903155352154.png)

![image-20250903155412456](assets/image-20250903155412456.png)

![image-20250903155507153](assets/image-20250903155507153.png)

![image-20250903155548265](assets/image-20250903155548265.png)

![image-20250903155620291](assets/image-20250903155620291.png)

![image-20250903155641562](assets/image-20250903155641562.png)

## idl_node

![image-20250903161311838](assets/image-20250903161311838.png)

![image-20250903161321044](assets/image-20250903161321044.png)

![image-20250903161402453](assets/image-20250903161402453.png)

![image-20250903161454747](assets/image-20250903161454747.png)

![image-20250903161524590](assets/image-20250903161524590.png)

![image-20250903161552687](assets/image-20250903161552687.png)

![image-20250903161626913](assets/image-20250903161626913.png)

![image-20250903161704403](assets/image-20250903161704403.png)

![image-20250903161738049](assets/image-20250903161738049.png)

## idl_parser

![image-20250903162520384](assets/image-20250903162520384.png)

![image-20250903162546028](assets/image-20250903162546028.png)

![image-20250903162609863](assets/image-20250903162609863.png)

![image-20250903162630854](assets/image-20250903162630854.png)

![image-20250903162704694](assets/image-20250903162704694.png)

![image-20250903162741973](assets/image-20250903162741973.png)

![image-20250903162803005](assets/image-20250903162803005.png)

![image-20250903162848092](assets/image-20250903162848092.png)

![image-20250903162927995](assets/image-20250903162927995.png)

![image-20250903163005814](assets/image-20250903163005814.png)

![image-20250903163450082](assets/image-20250903163450082.png)

![image-20250903163516631](assets/image-20250903163516631.png)

![image-20250903163559389](assets/image-20250903163559389.png)

![image-20250903163637429](assets/image-20250903163637429.png)

![image-20250903163709904](assets/image-20250903163709904.png)

![image-20250903163744994](assets/image-20250903163744994.png)

![image-20250903163807624](assets/image-20250903163807624.png)

![image-20250903163846940](assets/image-20250903163846940.png)

![image-20250903163910968](assets/image-20250903163910968.png)

![image-20250903163935126](assets/image-20250903163935126.png)

![image-20250903164003343](assets/image-20250903164003343.png)

## idl_semantic

![image-20250903164223371](assets/image-20250903164223371.png)

![image-20250903164303395](assets/image-20250903164303395.png)

![image-20250903164330989](assets/image-20250903164330989.png)

![image-20250903164348398](assets/image-20250903164348398.png)

## 实习工作描述

做了一个 **WebIDL 方言** 的编译前端：用 **PLY(lex/yacc)** 把 IDL 源码解析成 **AST**（`IDLNode`），再生成 **IR(JSON 风格字典)**，最后做一遍 **语义分析**（补充同步/异步、对象使用关系等标注），为后续代码生成做准备。

### 整体流水线（从文件到可用 IR）

1. **词法分析（Lexer, PLY-lex）**

   - 定义 token 集合和正则（关键字、标点、标识符、数字、注释等）。
   - 处理空白行号、非法字符；同时实现“特殊/普通注释”的抽取工具，后面会挂到 AST 属性里。

2. **语法分析（Parser, PLY-yacc）**

   - 在 `p_*` 函数的 **docstring** 里用 BNF 写规则（Interface/Mixin/Partial/Enum/Dictionary/Operation/Arguments…）。
   - 在每条规则归约时，构建 **AST 结点 `IDLNode`** 或临时属性对象 `IDLAttribute`，并把它们按父子关系挂起来。
   - 做了几类 **错误恢复** / 友好化处理：
     - `ERROR_REMAP` 把常见 “Unexpected … after …” 的底层错误映射成更友好的提示；
     - 针对 `InterfaceMembers`、`Arguments` 等关键非终结符写了 `…Error` 规则做恢复。

3. **AST 设计（idl_node.py）**

   - 通用结点类型 `IDLNode`：
     - 内部存 `properties` 字典（`NAME/TYPE/VALUE/ERRORS/WARNINGS/FILENAME/LINENO/POSITION`…）；
     - 提供 `AddChildren/SetProperty/GetProperty/Traverse/Tree` 等通用操作；
     - 提供大量 `IsA*()` 辅助判断（`IsAInterface/IsADictionary/IsAEnum/IsAType/IsAKey/IsAExtAttributes`…）。
   - 还有 `IDLSearch` 小类用于遍历和 pretty dump。

4. **IR 生成（idl_ir_generator.py）**

   - 输入 AST 的根结点（`File`），输出一个 **结构化 IR(dict)**：

     ```
     {
       "objectMap": {...},     # 各 Dictionary/Interface 的字段和类型
       "enumMap": {...},       # 枚举及成员
       "callbackMap": {...},   # 回调类型签名
       "config": config,       # 透传配置
       "apis": [...]           # 从 Interface 提取出来的 API 列表
     }
     ```

   - 核心方法：

     - `__generate_ir` 分派到 `dict/enum/interface/callback` 专用生成函数；
     - `__generate_interface_ir` 收集接口名、注释、扩展属性、以及 **Operation** 列表；
     - `__generate_operation_ir` 解析 `comment/arguments/returnType/extAttributesMap`;
     - `__generate_type_ir` 处理 `Typeref/PrimitiveType/FrozenArray/Any/list[...]` 嵌套；
     - `__generate_default_ir` 与 `__pretty_format` 处理常量/默认值(字符串加引号、浮点补 `.0` 等)。

   - 可选落盘：当传入 `dump_ir=True + dump_dir`，把 IR 按 `filename.json` 写到磁盘（`ensure_ascii=False, indent=4`）。

5. **语义分析（idl_semantic_analyser.py）**

   - 输入 **raw IR**，补充/推断额外语义：
     - **同步/异步**：
       - 若扩展属性 `Call` 指定 `asyncCall/syncCall`，直接用；
       - 否则扫描参数是否包含回调类型，**有回调 => 异步**，否则视为同步。
       - 结果写入 `api['isSync']`。
     - **对象使用标记**：
       - 对 **同步 API**：递归分析 **返回类型**，把涉及的对象在 `objectMap` 上标记 `usedInAPIReturn=True`；
       - 对 **异步 API**：递归分析 **回调参数** 的类型，同样做标记；
       - 递归函数 `__recursive_search_object_names(type_name)` 会展开 `list[Type]`、对象成员的嵌套引用。
   - 这一步的产物是“可用于后续代码生成/裁剪”的 **富 IR**。

6. **编译器入口（idl_compiler / parser 集成）**

   - `IDLLexer()` + `yacc.yacc(module=self)` 组装；
   - `ParseText(filename, data)`：先 `lexer.Tokenize`，再 `yacc.parse(lexer=self.lexer)` 得到 AST children，最后包一层 `File` 结点；
   - 交给 `IDLIRGenerator.generate()` 得到 IR，再喂给 `IDLSemanticAnalyser.analyse()` 产出最终 IR；
   - （可选）把 IR dump 成 JSON，供调试或生成器消费。

### 你具体解决了哪些工程问题

- **语法覆盖面**：支持 Interface / Mixin / Partial / Dictionary / Enum / Typedef / Includes / Const / Operation / Arguments / DefaultValue / ExtAttributes 等核心语法单元。
- **注释和扩展属性**：
  - 实现 `ExtractSpecialComment/ExtractComment`，把 `/** … */` 和 `//` 注释提取为结构化信息；
  - `DivideExtAttrsIntoApplicableAndNonApplicable` 把扩展属性分成“可作用于类型/不可作用于类型”两组，后续生成时放入 `extAttributesMap`。
- **错误体验**：除了 `p_error`，还用 `ERROR_REMAP` 提升报错可读性，并在多个非终结符上写了 `…Error` 规则做错误恢复，提升大文件的容错率。
- **IR 规范化**：
  - 统一了 Type 表达（`Typeref/Primitive/FrozenArray/list[...]`）；
  - 默认值做了统一格式化；
  - Interface 的 API 被“拍平”为 `apis` 列表，方便后续生成与统计。
- **语义增强**：自动判断 API 调用方式（同步/异步），并从返回值/回调参数反向标记被“暴露”的对象（可用于后续按需裁剪/生成更小代码）。
- 

### 面试官可能会追问 → 你可以这样答

- **为什么选 PLY？**
   纯 Python、LALR(1) 性能够用；规则写在函数 docstring，团队容易读；生态简单，易集成。
- **`t_\*` / `p_\*` 分别做什么？**
   `t_*` 是正则切 token；`p_*` 写文法并在“归约”时产出 AST/IR 片段；真正的语法写在 `p_*` 的 docstring 里（BNF 风格）。
- **怎么把 lexer 和 parser 连起来？**
   `self.tokens = self.lexer.KnownTokens()` 把 token 列表交给 yacc；`yacc.yacc(module=self)` 扫描 `p_*`；解析时 `yacc.parse(lexer=self.lexer)` 让 parser 从你的 lexer 拉 token。
- **AST 到 IR 的映射关键点？**
   面向使用场景设计了 `objectMap/enumMap/callbackMap/apis`；对 Operation 统一抽出 `name/returnType/arguments/extAttributesMap/comment`；类型/默认值归一化，便于语言后端消费。
- **怎么判定异步？为什么要递归标记对象？**
   根据扩展属性优先判定；缺省回退到“是否出现回调类型”。递归标记能找出深层嵌套的对象依赖，避免遗漏（例如 `list<Event>`、`Event` 再引用别的对象）。

## 概念

> ## 🔹 1. 什么是终结符？
>
> - **终结符（Terminal）\**就是\**词法分析结束后，不能再继续拆分的最小单元**。
> - 它们对应的是 lexer 吐出的 **token**。
> - 比如在 IDL 里：
>   - 关键字 `interface`
>   - 标识符 `User`
>   - 符号 `{ } ( ) : ;`
>   - 类型 `int`、`string`
> - 这些就是终结符，因为在语法分析阶段，它们就是最原子的“砖块”，不会再分解。
>
> ------
>
> ## 🔹 2. 什么是非终结符？
>
> - **非终结符（Non-terminal）\**就是\**文法里定义的更高层的结构**，它由终结符或其他非终结符组合而成。
> - 比如在 IDL 文法里：
>   - `interface`（接口定义）
>   - `method_list`（方法列表）
>   - `method`（单个方法）
>   - `param_list`（参数列表）
> - 这些不是直接由 lexer 产生的，而是 parser 在规约时构造出来的结构。
>
> ------
>
> ## 🔹 3. 什么是归约（Reduce）？
>
> **归约**就是语法分析时的“折叠”操作：
>
> - 当 **一串终结符/非终结符** 恰好匹配某条文法规则的右边，parser 就可以把它们“归约”成左边的非终结符。
> - 同时调用你写的 `p_*` 函数，执行语义动作，计算 `p[0]` 的值。
>
> ------
>
> ## 🔹 4. 举个直观例子（算式）
>
> 文法：
>
> ```
> expr : expr PLUS term
> expr : term
> term : NUMBER
> ```
>
> 输入：`2 + 3`
>
> - lexer 输出 token 流：`NUMBER(2), PLUS, NUMBER(3)`
>    → 这些就是**终结符**。
> - parser 开始分析：
>   1. 看到 `NUMBER(2)`，根据规则 `term : NUMBER`，把它 **归约** 成一个 `term`（值是 2）。
>   2. `term` 又能匹配 `expr : term`，于是再 **归约** 成 `expr`（值还是 2）。
>   3. 接下来读到 `PLUS`，再读到 `NUMBER(3)` → 归约成 `term`（值是 3）。
>   4. 现在栈顶是 `expr PLUS term`，正好匹配规则 `expr : expr PLUS term`，再 **归约** 成一个新的 `expr`，值是 `2 + 3 = 5`。
>
> 最终整个输入归约成一个 `expr`，值为 5。
>
> ------
>
> ## 🔹 5. 对比到 IDL
>
> 输入：
>
> ```
> interface User { };
> ```
>
> - lexer 吐出终结符：`INTERFACE IDENT("User") { } ;`
>
> - parser 根据规则：
>
>   ```
>   interface : INTERFACE IDENT '{' method_list '}' ';'
>   ```
>
>   当它看到 `INTERFACE IDENT { } ;` 这一串，就会 **归约** 成一个 `interface` 非终结符，
>    并且在 `p_interface` 函数里，你会构造一个 AST 节点：
>
>   ```
>   p[0] = IDLNode("Interface", name=p[2], children=p[4])
>   ```
>
> ------
>
> ## 🔹 一句话记忆
>
> - **终结符**：lexer 切出来的最小单位（token）。
> - **非终结符**：parser 用文法定义的更大结构。
> - **归约**：当一串符号匹配文法右边时，把它们折叠成左边非终结符，并执行语义动作（`p[0] = ...`）。

## 面试如何讲

### 做的工作，如何把IDL转到IR

> 我做的工作是把我们内部定义的一种 IDL 文件转成统一的中间表示 IR，供后续多端代码生成和文档生成使用。
>
> 我一开始选择用 **Python** 实现，主要考虑两点：一是 Python 写原型开发效率高，二是有成熟的 **PLY** 库，它是 Python 版的 Lex/Yacc，支持词法分析和 LALR(1) 语法分析，很适合做这种小型编译前端。
>
> 整个流程我分成了四个模块：
>
> 1. **Lexer（词法分析）**
>    - 用 `t_*` 规则定义关键字、标识符、内置类型和符号。
>    - 例如 `interface` 会被识别成 `INTERFACE` token，标识符识别成 `IDENT`，符号如 `{` `}` 直接作为 literals。
>    - 这一层就是把字符流切成 token 流。
> 2. **Parser（语法分析）**
>    - 用 `p_*` 函数写语法规则，docstring 里用 BNF 格式描述，比如 `interface : INTERFACE IDENT '{' method_list '}' ';'`。
>    - 当某条规则被归约时，对应的 `p_*` 会被调用，我就在函数体里构建 AST 节点（`IDLNode`）。
>    - 这样就能把 token 流还原成抽象语法树，树里保存了接口、方法、参数等结构信息。
> 3. **IR Generator（中间表示生成）**
>    - 遍历 AST，把接口、方法、参数、返回类型、扩展属性等抽取出来，生成统一的 JSON 风格 IR。
>    - 设计了几个核心部分：`objectMap` 存字典和对象，`enumMap` 存枚举，`apis` 存方法列表。
>    - 同时对类型和默认值做了格式化，比如 `list[int]` 这样的复合类型也能规范输出。
> 4. **Semantic Analyser（语义分析）**
>    - 在 IR 基础上再做一层增强。比如根据扩展属性或参数里是否有回调，自动判断方法是同步还是异步。
>    - 还会递归分析返回值和回调参数的类型，标记哪些对象在 API 返回中被使用，用于后续代码生成的裁剪。
>
> 通过这四步，整个编译流程就是：
>  **IDL 源文件 → Lexer → Parser(AST) → IR Generator → Semantic Analyser → 最终 IR**。
>
> 我们先把源字符流喂给Lexer，把字符流切成token流，然后交给parser根据文法进行归约，并且构建更大的结构(AST).
>
> Parser 之后得到的是 AST，但树结构对下游直接用还不够。
>  我写的 IR Generator 把 AST 转换成统一的 JSON 风格 IR，里面清楚标注了接口、方法、参数、返回值。
>  然后 Semantic Analyser 在这个 IR 上进一步增强，比如判断方法是同步还是异步、分析依赖对象、检查参数合法性。
>  最终得到的 IR 就是下游多端代码生成和文档生成的唯一数据源。

> 最后这套工具已经在内网合并使用，可以把一份 IDL 文件稳定地转成结构化 IR，下游的代码生成和文档生成都基于这个 IR 来保证一致性。
>
> 对我个人来说，最大的收获是把编译原理里的概念（词法、语法、归约、BNF）真正用在了工程中，同时也锻炼了如何把抽象规则落地成一个团队可用的工具链。

### IR Generator（中间表示生成）

> **目标**：把 AST（抽象语法树）转成统一、结构化的数据格式（通常是 JSON/dict），方便下游用。
>
> ### 在你的项目里：
>
> - parser 已经能生成 AST 节点（`IDLNode`），带有类型（Interface / Method / Enum / Param 等）和属性。
> - IR Generator 遍历 AST，把它映射到一个统一的数据结构（IR）。
>
> ### 常见做法：
>
> - **接口** → 放进 `interfaces` 表
> - **方法** → 每个方法变成一个 dict，包含名字、参数列表、返回值
> - **参数** → `{ "name": "age", "type": "int" }`
> - **枚举/字典** → 放进专门的 `enumMap` / `objectMap`
>
> ### 举例：
>
> 输入 IDL：
>
> ```
> interface User {
>   method getName() : string;
>   method setAge(age: int) : bool;
> };
> ```
>
> AST 是树状的：
>
> ```
> Interface(User)
>  ├─ Method(getName, returnType=string)
>  └─ Method(setAge, params=[Param(age,int)], returnType=bool)
> ```
>
> IR Generator 把它转成 JSON 风格：
>
> ```
> {
>   "interfaces": {
>     "User": {
>       "methods": [
>         {"name": "getName", "params": [], "returnType": "string"},
>         {"name": "setAge", "params": [{"name": "age", "type": "int"}], "returnType": "bool"}
>       ]
>     }
>   }
> }
> ```
>
> 📌 **关键点**：IR Generator 做的事就是“把树转成结构化表格”，统一格式，方便后面生成代码、生成文档。

### Semantic Analyser（语义分析器）

> **目标**：在 IR 基础上，做“超出语法的检查和补充”，确保 IR 更有语义价值。
>
> ### 在你的项目里：
>
> - **判定方法类型**：
>   - 如果方法有 callback 参数 → 标记为异步；否则默认同步。
> - **依赖分析**：
>   - 方法返回类型是对象/枚举 → 递归标记这些对象在 API 中“被使用”。
> - **一致性检查**：
>   - 比如参数重复、方法重名、未声明的类型 → 抛出错误。
>
> ### 举例：
>
> 假设有个接口：
>
> ```
> interface Chat {
>   method sendMessage(msg: string, cb: Callback) : bool;
> };
> ```
>
> IR Generator 生成的 IR：
>
> ```
> {
>   "interfaces": {
>     "Chat": {
>       "methods": [
>         {
>           "name": "sendMessage",
>           "params": [
>             {"name": "msg", "type": "string"},
>             {"name": "cb", "type": "Callback"}
>           ],
>           "returnType": "bool"
>         }
>       ]
>     }
>   }
> }
> ```
>
> Semantic Analyser 会再加语义信息：
>
> ```
> {
>   "interfaces": {
>     "Chat": {
>       "methods": [
>         {
>           "name": "sendMessage",
>           "params": [...],
>           "returnType": "bool",
>           "isAsync": true   // 语义分析阶段标注
>         }
>       ]
>     }
>   },
>   "usedObjects": ["Callback"]   // 递归标记依赖
> }
> ```
>
> 📌 **关键点**：Semantic Analyser 就像 QA + 增强器：
>
> - QA（检查合法性）；
> - 增强器（增加 `isAsync`、`usedObjects` 这些语义标签）。
